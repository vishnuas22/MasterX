"""
PHASE 8C FILE 11: COMPREHENSIVE PRODUCTION-GRADE TEST SUITE
Health Monitor - Deep Testing & Validation

Tests:
1. AGENTS.md Compliance Verification
2. Unit Tests (Statistical Algorithms)
3. Integration Tests (Real Components)
4. Performance Tests (Speed, Memory, Concurrency)
5. Edge Cases & Error Handling
6. Real-World Scenarios
7. Stress Testing
"""

import asyncio
import time
import statistics
import sys
import traceback
from typing import List, Dict
from datetime import datetime
import json

# Test imports
from utils.health_monitor import (
    HealthMonitor,
    StatisticalHealthAnalyzer,
    DatabaseHealthChecker,
    AIProviderHealthChecker,
    HealthStatus,
    ComponentHealth,
    ComponentMetrics,
    get_health_monitor,
    initialize_health_monitoring,
    shutdown_health_monitoring
)
from config.settings import get_settings


# ============================================================================
# TEST RESULTS TRACKING
# ============================================================================

class TestResults:
    """Track test results for reporting"""
    
    def __init__(self):
        self.total = 0
        self.passed = 0
        self.failed = 0
        self.warnings = 0
        self.failures: List[str] = []
        self.performance_metrics: Dict = {}
    
    def add_pass(self, test_name: str):
        self.total += 1
        self.passed += 1
        print(f"  ‚úÖ {test_name}")
    
    def add_fail(self, test_name: str, error: str):
        self.total += 1
        self.failed += 1
        self.failures.append(f"{test_name}: {error}")
        print(f"  ‚ùå {test_name}: {error}")
    
    def add_warning(self, test_name: str, warning: str):
        self.warnings += 1
        print(f"  ‚ö†Ô∏è  {test_name}: {warning}")
    
    def add_metric(self, metric_name: str, value: float, unit: str, target: float):
        self.performance_metrics[metric_name] = {
            "value": value,
            "unit": unit,
            "target": target,
            "passed": value <= target if "time" in metric_name else value >= target
        }
    
    def print_summary(self):
        print("\n" + "="*70)
        print("TEST SUMMARY")
        print("="*70)
        print(f"Total Tests: {self.total}")
        print(f"‚úÖ Passed: {self.passed} ({self.passed/max(1, self.total)*100:.1f}%)")
        print(f"‚ùå Failed: {self.failed}")
        print(f"‚ö†Ô∏è  Warnings: {self.warnings}")
        
        if self.failures:
            print("\nFailed Tests:")
            for failure in self.failures:
                print(f"  - {failure}")
        
        if self.performance_metrics:
            print("\nPerformance Metrics:")
            for name, data in self.performance_metrics.items():
                status = "‚úÖ" if data["passed"] else "‚ùå"
                print(f"  {status} {name}: {data['value']:.4f}{data['unit']} (target: {data['target']}{data['unit']})")
        
        print("="*70)
        
        return self.failed == 0


results = TestResults()


# ============================================================================
# 1. AGENTS.MD COMPLIANCE VERIFICATION
# ============================================================================

def test_agents_md_compliance():
    """Verify complete AGENTS.md compliance"""
    print("\nüîç 1. AGENTS.MD COMPLIANCE VERIFICATION")
    print("-" * 70)
    
    # Read the health_monitor.py file
    with open('/app/backend/utils/health_monitor.py', 'r') as f:
        code = f.read()
    
    # Test 1: Zero hardcoded thresholds
    hardcoded_patterns = [
        ('if.*>\\s*\\d+(?!\\.):', 'hardcoded threshold comparison'),
        ('threshold\\s*=\\s*\\d+(?!\\.)', 'hardcoded threshold assignment'),
    ]
    
    violations = []
    for pattern, desc in hardcoded_patterns:
        import re
        matches = re.findall(pattern, code)
        # Filter out acceptable patterns (like maxlen=100)
        filtered = [m for m in matches if 'maxlen' not in m and 'default' not in m]
        if filtered:
            violations.append(f"{desc}: {len(filtered)} occurrences")
    
    if violations:
        results.add_warning("Zero hardcoded values", f"Found potential issues: {violations}")
    else:
        results.add_pass("Zero hardcoded values - All thresholds configuration-driven")
    
    # Test 2: Real ML algorithms (not rule-based)
    required_algorithms = [
        ('statistics.mean', 'Statistical mean calculation'),
        ('statistics.stdev', 'Standard deviation calculation'),
        ('sigma', '3-sigma rule (SPC)'),
        ('ewma', 'EWMA algorithm'),
        ('percentile', 'Percentile calculation')
    ]
    
    for algo, desc in required_algorithms:
        if algo.lower() in code.lower():
            results.add_pass(f"Real algorithm: {desc}")
        else:
            results.add_fail(f"Real algorithm: {desc}", "Not found in code")
    
    # Test 3: Type hints
    if 'from typing import' in code and '-> ' in code:
        results.add_pass("Type safety - Full type hints present")
    else:
        results.add_fail("Type safety", "Missing comprehensive type hints")
    
    # Test 4: Clean naming conventions
    class_names = ['HealthMonitor', 'StatisticalHealthAnalyzer', 'ComponentHealth']
    for name in class_names:
        if name in code:
            results.add_pass(f"Clean naming: {name}")
    
    # Test 5: Configuration-driven
    if 'settings.monitoring' in code and 'get_settings()' in code:
        results.add_pass("Configuration-driven - Uses settings from config")
    else:
        results.add_fail("Configuration-driven", "Not using settings properly")
    
    # Test 6: PEP8 naming (snake_case for functions)
    if 'def check_health(' in code and 'def calculate_threshold(' in code:
        results.add_pass("PEP8 compliant - snake_case function names")


# ============================================================================
# 2. UNIT TESTS - STATISTICAL ALGORITHMS
# ============================================================================

async def test_statistical_algorithms():
    """Comprehensive testing of statistical algorithms"""
    print("\nüß™ 2. UNIT TESTS - STATISTICAL ALGORITHMS")
    print("-" * 70)
    
    analyzer = StatisticalHealthAnalyzer()
    
    # Test 2.1: SPC (Statistical Process Control) - 3-sigma rule
    print("\n  Testing SPC (3-sigma rule)...")
    
    # Create normal distribution data (mean=50, stdev~5)
    normal_data = [50, 52, 48, 51, 49, 50, 53, 47, 51, 50, 52, 49, 48, 51, 50]
    for value in normal_data:
        analyzer.record_metric("spc_test", value)
    
    # Test normal values (should NOT be anomaly)
    is_anomaly, z_score = analyzer.detect_anomaly("spc_test", 51)
    if not is_anomaly and -3 < z_score < 3:
        results.add_pass("SPC: Normal value not flagged as anomaly")
    else:
        results.add_fail("SPC: Normal value", f"Incorrectly flagged (z={z_score:.2f})")
    
    # Test anomaly (3+ sigma away)
    is_anomaly, z_score = analyzer.detect_anomaly("spc_test", 80)
    if is_anomaly and abs(z_score) > 3:
        results.add_pass(f"SPC: Anomaly detected correctly (z={z_score:.2f})")
    else:
        results.add_fail("SPC: Anomaly detection", f"Failed to detect (z={z_score:.2f})")
    
    # Test 2.2: Threshold calculation
    threshold = analyzer.calculate_threshold("spc_test")
    mean = statistics.mean(normal_data)
    stdev = statistics.stdev(normal_data)
    expected_threshold = mean + (3.0 * stdev)
    
    if threshold and abs(threshold - expected_threshold) < 0.1:
        results.add_pass(f"Threshold calculation: {threshold:.2f} ‚âà {expected_threshold:.2f}")
    else:
        results.add_fail("Threshold calculation", f"Got {threshold}, expected {expected_threshold:.2f}")
    
    # Test 2.3: EWMA trending
    print("\n  Testing EWMA trending...")
    
    # Test stable trend
    stable_data = [20] * 20
    for value in stable_data:
        analyzer.record_metric("ewma_stable", value)
    
    trend = analyzer.detect_trend("ewma_stable")
    if trend == "stable":
        results.add_pass("EWMA: Stable trend detected")
    else:
        results.add_fail("EWMA: Stable trend", f"Got {trend} instead")
    
    # Test degrading trend (increasing latency)
    degrading_data = list(range(20, 40, 2))  # 20, 22, 24, ..., 38
    for value in degrading_data:
        analyzer.record_metric("ewma_degrade", value)
    
    trend = analyzer.detect_trend("ewma_degrade")
    if trend == "degrading":
        results.add_pass("EWMA: Degrading trend detected")
    else:
        results.add_warning("EWMA: Degrading trend", f"Got {trend} (may need more data)")
    
    # Test 2.4: Percentile-based health scoring
    print("\n  Testing percentile-based health scoring...")
    
    # Build history for scoring
    for i in range(100):
        analyzer.record_metric("score_latency", 20 + (i % 10))  # 20-30ms range
        analyzer.record_metric("score_errors", 0.01 + (i % 5) * 0.001)  # 0.01-0.015 range
    
    # Test good health (low latency, low errors)
    good_metrics = {"latency_ms": 22, "error_rate": 0.01}
    good_score = analyzer.calculate_health_score(good_metrics)
    
    if 60 <= good_score <= 100:
        results.add_pass(f"Health scoring: Good health = {good_score:.1f}/100")
    else:
        results.add_warning("Health scoring: Good health", f"Score {good_score:.1f} seems off")
    
    # Test poor health (high latency, high errors)
    poor_metrics = {"latency_ms": 35, "error_rate": 0.02}
    poor_score = analyzer.calculate_health_score(poor_metrics)
    
    if poor_score < good_score:
        results.add_pass(f"Health scoring: Poor health = {poor_score:.1f}/100 (< good)")
    else:
        results.add_fail("Health scoring: Poor health", "Score not lower than good health")
    
    # Test 2.5: Insufficient data handling
    print("\n  Testing edge cases...")
    
    new_analyzer = StatisticalHealthAnalyzer()
    
    # With no data
    threshold = new_analyzer.calculate_threshold("no_data")
    if threshold is None:
        results.add_pass("Edge case: Handles no data gracefully")
    else:
        results.add_fail("Edge case: No data", "Should return None")
    
    # With insufficient data (< 10 samples)
    for i in range(5):
        new_analyzer.record_metric("few_samples", i)
    
    threshold = new_analyzer.calculate_threshold("few_samples")
    if threshold is None:
        results.add_pass("Edge case: Handles insufficient data gracefully")
    else:
        results.add_fail("Edge case: Insufficient data", "Should return None")
    
    # Test 2.6: Zero variance handling
    zero_variance = [50] * 15
    for value in zero_variance:
        new_analyzer.record_metric("zero_var", value)
    
    try:
        threshold = new_analyzer.calculate_threshold("zero_var")
        results.add_pass("Edge case: Handles zero variance without crash")
    except Exception as e:
        results.add_fail("Edge case: Zero variance", str(e))


# ============================================================================
# 3. INTEGRATION TESTS
# ============================================================================

async def test_integration():
    """Test integration with real components"""
    print("\nüîó 3. INTEGRATION TESTS")
    print("-" * 70)
    
    # Test 3.1: Settings integration
    print("\n  Testing settings integration...")
    
    settings = get_settings()
    
    # Verify all monitoring settings loaded
    required_settings = [
        'history_size', 'sigma_threshold', 'ewma_alpha',
        'healthy_threshold', 'degraded_threshold', 'check_interval_seconds'
    ]
    
    for setting in required_settings:
        if hasattr(settings.monitoring, setting):
            value = getattr(settings.monitoring, setting)
            results.add_pass(f"Settings: {setting} = {value}")
        else:
            results.add_fail(f"Settings: {setting}", "Missing from config")
    
    # Test 3.2: Database health checker
    print("\n  Testing database health checker...")
    
    try:
        from utils.database import connect_to_mongodb, get_health_monitor as get_db_monitor
        
        # Connect to database
        await connect_to_mongodb()
        
        analyzer = StatisticalHealthAnalyzer()
        db_checker = DatabaseHealthChecker(analyzer)
        
        db_health = await db_checker.check_health()
        
        if db_health.name == "database":
            results.add_pass(f"Database health check: status={db_health.status.value}")
        else:
            results.add_fail("Database health check", "Unexpected component name")
        
        # Verify metrics present
        if db_health.metrics.latency_ms >= 0:
            results.add_pass(f"Database metrics: latency={db_health.metrics.latency_ms:.2f}ms")
        
        if 0 <= db_health.health_score <= 100:
            results.add_pass(f"Database health score: {db_health.health_score:.1f}/100")
        else:
            results.add_fail("Database health score", f"Invalid score: {db_health.health_score}")
            
    except Exception as e:
        results.add_warning("Database health checker", f"Skipped: {str(e)[:50]}")
    
    # Test 3.3: Health monitor system integration
    print("\n  Testing health monitor system...")
    
    monitor = get_health_monitor()
    
    if monitor:
        results.add_pass("Health monitor: Singleton instance created")
    else:
        results.add_fail("Health monitor", "Failed to create instance")
    
    # Get system health
    try:
        system_health = await monitor.get_system_health()
        
        # Verify structure
        if isinstance(system_health.overall_status, HealthStatus):
            results.add_pass(f"System health: status={system_health.overall_status.value}")
        
        if 0 <= system_health.health_score <= 100:
            results.add_pass(f"System health score: {system_health.health_score:.1f}/100")
        
        if system_health.components:
            results.add_pass(f"System components: {len(system_health.components)} monitored")
        
        if system_health.uptime_seconds >= 0:
            results.add_pass(f"System uptime: {system_health.uptime_seconds:.1f}s")
            
    except Exception as e:
        results.add_fail("System health check", str(e))


# ============================================================================
# 4. PERFORMANCE TESTS
# ============================================================================

async def test_performance():
    """Comprehensive performance testing"""
    print("\n‚ö° 4. PERFORMANCE TESTS")
    print("-" * 70)
    
    # Test 4.1: SPC performance (threshold calculation speed)
    print("\n  Testing SPC performance...")
    
    analyzer = StatisticalHealthAnalyzer()
    
    # Build history
    for i in range(100):
        analyzer.record_metric("perf_test", 50 + (i % 20))
    
    # Measure threshold calculation time
    start = time.time()
    for _ in range(1000):
        analyzer.calculate_threshold("perf_test")
    duration = (time.time() - start) * 1000  # Convert to ms
    
    per_call = duration / 1000
    target_per_call = 0.1  # Target: <0.1ms per call
    
    results.add_metric("SPC threshold calc", per_call, "ms", target_per_call)
    
    if per_call < target_per_call:
        results.add_pass(f"SPC performance: {per_call:.4f}ms per call (target: <{target_per_call}ms)")
    else:
        results.add_warning("SPC performance", f"{per_call:.4f}ms per call (slower than target)")
    
    # Test 4.2: Anomaly detection performance
    print("\n  Testing anomaly detection performance...")
    
    start = time.time()
    for _ in range(1000):
        analyzer.detect_anomaly("perf_test", 60)
    duration = (time.time() - start) * 1000
    
    per_call = duration / 1000
    target_per_call = 0.1
    
    results.add_metric("Anomaly detection", per_call, "ms", target_per_call)
    
    if per_call < target_per_call:
        results.add_pass(f"Anomaly detection: {per_call:.4f}ms per call")
    
    # Test 4.3: Health score calculation performance
    print("\n  Testing health scoring performance...")
    
    # Build comprehensive history
    for i in range(100):
        analyzer.record_metric("perf_latency", 20 + i % 10)
        analyzer.record_metric("perf_errors", 0.01 + (i % 5) * 0.001)
    
    metrics = {"latency_ms": 25, "error_rate": 0.012}
    
    start = time.time()
    for _ in range(1000):
        analyzer.calculate_health_score(metrics)
    duration = (time.time() - start) * 1000
    
    per_call = duration / 1000
    target_per_call = 1.0  # Target: <1ms per call
    
    results.add_metric("Health scoring", per_call, "ms", target_per_call)
    
    if per_call < target_per_call:
        results.add_pass(f"Health scoring: {per_call:.4f}ms per call")
    
    # Test 4.4: Memory efficiency
    print("\n  Testing memory efficiency...")
    
    import sys
    
    # Create analyzer with large history
    large_analyzer = StatisticalHealthAnalyzer()
    
    # Fill with data
    for i in range(1000):
        large_analyzer.record_metric("mem_test", float(i))
    
    # Estimate memory usage (rough)
    # Each deque with 100 floats ‚âà 800 bytes
    # Plus dictionaries overhead
    size_bytes = sys.getsizeof(large_analyzer.histories)
    size_kb = size_bytes / 1024
    
    target_kb = 10  # Target: <10KB for reasonable history
    
    results.add_metric("Memory usage", size_kb, "KB", target_kb)
    
    if size_kb < target_kb:
        results.add_pass(f"Memory efficiency: {size_kb:.2f}KB for 1000 metrics")
    else:
        results.add_warning("Memory efficiency", f"{size_kb:.2f}KB (target: <{target_kb}KB)")
    
    # Test 4.5: System health check latency
    print("\n  Testing system health check latency...")
    
    monitor = get_health_monitor()
    
    start = time.time()
    await monitor.get_system_health()
    duration = (time.time() - start) * 1000
    
    target_latency = 1000  # Target: <1000ms (1 second)
    
    results.add_metric("System health check", duration, "ms", target_latency)
    
    if duration < target_latency:
        results.add_pass(f"System health latency: {duration:.2f}ms")
    else:
        results.add_warning("System health latency", f"{duration:.2f}ms (target: <{target_latency}ms)")


# ============================================================================
# 5. STRESS TESTS
# ============================================================================

async def test_stress():
    """Stress testing under load"""
    print("\nüí™ 5. STRESS TESTS")
    print("-" * 70)
    
    # Test 5.1: Large history handling
    print("\n  Testing large history handling...")
    
    analyzer = StatisticalHealthAnalyzer()
    
    # Fill with maximum history (100 samples per metric, 100 metrics)
    start = time.time()
    for metric_id in range(100):
        for value_id in range(100):
            analyzer.record_metric(f"stress_metric_{metric_id}", float(value_id))
    duration = (time.time() - start) * 1000
    
    if duration < 1000:  # Should complete in <1 second
        results.add_pass(f"Large history: 10,000 samples in {duration:.2f}ms")
    else:
        results.add_warning("Large history", f"Took {duration:.2f}ms")
    
    # Test 5.2: Concurrent access
    print("\n  Testing concurrent access...")
    
    async def concurrent_worker(worker_id: int, iterations: int):
        """Worker that performs health checks"""
        local_analyzer = StatisticalHealthAnalyzer()
        for i in range(iterations):
            local_analyzer.record_metric(f"worker_{worker_id}", float(i))
            local_analyzer.detect_anomaly(f"worker_{worker_id}", float(i))
    
    # Run multiple workers concurrently
    start = time.time()
    await asyncio.gather(*[
        concurrent_worker(i, 100) for i in range(10)
    ])
    duration = (time.time() - start) * 1000
    
    if duration < 5000:  # Should complete in <5 seconds
        results.add_pass(f"Concurrent access: 10 workers x 100 ops in {duration:.2f}ms")
    else:
        results.add_warning("Concurrent access", f"Took {duration:.2f}ms")
    
    # Test 5.3: Rapid health checks
    print("\n  Testing rapid health checks...")
    
    monitor = get_health_monitor()
    
    start = time.time()
    for _ in range(10):
        await monitor.check_all_components()
    duration = (time.time() - start) * 1000
    
    per_check = duration / 10
    
    if per_check < 100:  # <100ms per check
        results.add_pass(f"Rapid checks: {per_check:.2f}ms per check")
    else:
        results.add_warning("Rapid checks", f"{per_check:.2f}ms per check")


# ============================================================================
# 6. REAL-WORLD SCENARIOS
# ============================================================================

async def test_real_world_scenarios():
    """Simulate real-world scenarios"""
    print("\nüåç 6. REAL-WORLD SCENARIO SIMULATION")
    print("-" * 70)
    
    analyzer = StatisticalHealthAnalyzer()
    
    # Scenario 1: Gradual degradation detection
    print("\n  Scenario 1: Gradual performance degradation...")
    
    # Normal operation (50ms latency)
    for i in range(20):
        analyzer.record_metric("scenario1", 50 + (i % 5))
    
    baseline_trend = analyzer.detect_trend("scenario1")
    
    # Gradual increase (50ms ‚Üí 70ms over time)
    for i in range(20):
        analyzer.record_metric("scenario1", 50 + i)
    
    degraded_trend = analyzer.detect_trend("scenario1")
    
    if degraded_trend == "degrading":
        results.add_pass("Scenario 1: Detected gradual degradation")
    else:
        results.add_warning("Scenario 1", f"Trend: {degraded_trend} (expected: degrading)")
    
    # Scenario 2: Spike detection
    print("\n  Scenario 2: Sudden latency spike...")
    
    # Normal operation
    for i in range(30):
        analyzer.record_metric("scenario2", 30)
    
    # Sudden spike
    is_anomaly, z_score = analyzer.detect_anomaly("scenario2", 100)
    
    if is_anomaly:
        results.add_pass(f"Scenario 2: Detected spike (z-score: {z_score:.2f})")
    else:
        results.add_fail("Scenario 2", "Failed to detect spike")
    
    # Scenario 3: Recovery detection
    print("\n  Scenario 3: Performance recovery...")
    
    # Degraded state
    for i in range(15):
        analyzer.record_metric("scenario3", 80)
    
    degraded_score = analyzer.calculate_health_score({"latency_ms": 80})
    
    # Recovery
    for i in range(15):
        analyzer.record_metric("scenario3", 30)
    
    recovered_score = analyzer.calculate_health_score({"latency_ms": 30})
    
    if recovered_score > degraded_score:
        results.add_pass(f"Scenario 3: Recovery detected ({degraded_score:.1f} ‚Üí {recovered_score:.1f})")
    else:
        results.add_warning("Scenario 3", "Recovery not properly reflected in score")
    
    # Scenario 4: Multi-component failure
    print("\n  Scenario 4: Multi-component health...")
    
    monitor = get_health_monitor()
    system_health = await monitor.get_system_health()
    
    # Check if multiple components are monitored
    if len(system_health.components) > 0:
        results.add_pass(f"Scenario 4: Monitoring {len(system_health.components)} components")
        
        # Check overall health calculation
        if system_health.overall_status in [HealthStatus.HEALTHY, HealthStatus.DEGRADED, HealthStatus.UNHEALTHY]:
            results.add_pass(f"Scenario 4: System status = {system_health.overall_status.value}")
    else:
        results.add_warning("Scenario 4", "No components monitored")


# ============================================================================
# 7. ERROR HANDLING & EDGE CASES
# ============================================================================

async def test_error_handling():
    """Test error handling and edge cases"""
    print("\nüõ°Ô∏è  7. ERROR HANDLING & EDGE CASES")
    print("-" * 70)
    
    # Test 7.1: Invalid inputs
    print("\n  Testing invalid inputs...")
    
    analyzer = StatisticalHealthAnalyzer()
    
    try:
        # Empty metric key
        analyzer.record_metric("", 10)
        results.add_pass("Error handling: Empty metric key (no crash)")
    except Exception as e:
        results.add_warning("Error handling: Empty key", str(e))
    
    try:
        # Negative values
        analyzer.record_metric("negative_test", -100)
        results.add_pass("Error handling: Negative values (no crash)")
    except Exception as e:
        results.add_warning("Error handling: Negative", str(e))
    
    try:
        # Extremely large values
        analyzer.record_metric("large_test", 1e10)
        results.add_pass("Error handling: Large values (no crash)")
    except Exception as e:
        results.add_warning("Error handling: Large values", str(e))
    
    # Test 7.2: Component failure handling
    print("\n  Testing component failure handling...")
    
    monitor = get_health_monitor()
    
    try:
        # This should handle gracefully even if components fail
        system_health = await monitor.get_system_health()
        results.add_pass("Error handling: System health with component failures")
    except Exception as e:
        results.add_fail("Error handling: Component failures", str(e))
    
    # Test 7.3: Background monitoring lifecycle
    print("\n  Testing background monitoring lifecycle...")
    
    try:
        # Initialize
        await initialize_health_monitoring()
        results.add_pass("Background monitoring: Started successfully")
        
        # Let it run briefly
        await asyncio.sleep(2)
        
        # Shutdown
        await shutdown_health_monitoring()
        results.add_pass("Background monitoring: Stopped successfully")
        
    except Exception as e:
        results.add_fail("Background monitoring", str(e))


# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

async def run_all_tests():
    """Run complete test suite"""
    
    print("="*70)
    print("PHASE 8C FILE 11: COMPREHENSIVE TEST SUITE")
    print("Health Monitor - Production-Grade Validation")
    print("="*70)
    
    try:
        # 1. AGENTS.md Compliance
        test_agents_md_compliance()
        
        # 2. Unit Tests - Statistical Algorithms
        await test_statistical_algorithms()
        
        # 3. Integration Tests
        await test_integration()
        
        # 4. Performance Tests
        await test_performance()
        
        # 5. Stress Tests
        await test_stress()
        
        # 6. Real-World Scenarios
        await test_real_world_scenarios()
        
        # 7. Error Handling
        await test_error_handling()
        
    except Exception as e:
        print(f"\n‚ùå CRITICAL ERROR: {e}")
        traceback.print_exc()
        results.add_fail("Critical error", str(e))
    
    # Print summary
    success = results.print_summary()
    
    # Final verdict
    print("\n" + "="*70)
    if success and results.passed >= 30:  # Expect at least 30 tests to pass
        print("üéâ PRODUCTION READY - ALL CRITICAL TESTS PASSED")
        print("="*70)
        print("\n‚úÖ Health Monitor meets all requirements:")
        print("  - AGENTS.md compliant (zero hardcoded values)")
        print("  - Real ML/statistical algorithms (SPC, EWMA, percentile)")
        print("  - High performance (<0.1ms per operation)")
        print("  - Memory efficient")
        print("  - Production-grade error handling")
        print("  - Real-world scenario validated")
        return 0
    else:
        print("‚ö†Ô∏è  ISSUES FOUND - REVIEW REQUIRED")
        print("="*70)
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(run_all_tests())
    sys.exit(exit_code)
