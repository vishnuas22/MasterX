#!/usr/bin/env python3
"""
üöÄ COMPREHENSIVE QUANTUM INTELLIGENCE TESTING SUITE V6.0
Real-world testing with multiple learning scenarios and performance validation

Tests:
1. Multi-provider AI interaction tests
2. Real-time performance metrics validation
3. Empathy and emotion detection testing  
4. Difficulty adjustment and adaptation
5. Learning pipeline optimization
6. Context management efficiency
"""

import asyncio
import json
import time
import aiohttp
from typing import Dict, List, Any
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuantumIntelligenceValidator:
    """Comprehensive validation of quantum intelligence features"""
    
    def __init__(self):
        self.base_url = "http://localhost:8001/api"
        self.test_results = []
        self.performance_metrics = []
        self.user_scenarios = [
            {
                "user_id": "student_beginner_001",
                "profile": "beginner",
                "scenario": "struggling_with_math",
                "expected_empathy": 0.8,
                "expected_difficulty": "easy"
            },
            {
                "user_id": "student_advanced_002", 
                "profile": "advanced",
                "scenario": "complex_problem_solving",
                "expected_empathy": 0.6,
                "expected_difficulty": "challenging"
            },
            {
                "user_id": "student_emotional_003",
                "profile": "stressed",
                "scenario": "exam_anxiety",
                "expected_empathy": 0.9,
                "expected_difficulty": "moderate"
            }
        ]
        
    async def run_comprehensive_tests(self):
        """Run all quantum intelligence tests"""
        print("üöÄ Starting Comprehensive Quantum Intelligence Tests...")
        print("=" * 70)
        
        # Test 1: Basic System Health
        await self.test_system_health()
        
        # Test 2: Multi-Provider AI Integration
        await self.test_ai_provider_integration()
        
        # Test 3: Real Learning Scenarios
        await self.test_real_learning_scenarios()
        
        # Test 4: Performance Metrics Validation
        await self.test_performance_metrics()
        
        # Test 5: Empathy and Emotion Detection
        await self.test_empathy_detection()
        
        # Test 6: Difficulty Adaptation
        await self.test_difficulty_adaptation()
        
        # Test 7: Context Management
        await self.test_context_management()
        
        # Test 8: Real-Time Optimization
        await self.test_optimization_systems()
        
        # Generate Comprehensive Report
        await self.generate_test_report()
    
    async def test_system_health(self):
        """Test basic system health and quantum intelligence availability"""
        print("\nüîç Test 1: System Health Check")
        print("-" * 40)
        
        try:
            async with aiohttp.ClientSession() as session:
                # Basic health check
                async with session.get(f"{self.base_url}/") as resp:
                    data = await resp.json()
                    print(f"‚úÖ System Status: {data['status']}")
                    print(f"‚úÖ Version: {data['version']}")
                    
                # Check quantum intelligence endpoint
                health_endpoint = f"{self.base_url}/quantum/health"
                try:
                    async with session.get(health_endpoint) as resp:
                        if resp.status == 200:
                            health_data = await resp.json()
                            print(f"‚úÖ Quantum Intelligence Health: {health_data.get('status', 'Unknown')}")
                        else:
                            print(f"‚ö†Ô∏è Quantum health endpoint returned: {resp.status}")
                except:
                    print("‚ÑπÔ∏è Quantum health endpoint not available - will test via message processing")
                
                self.test_results.append({
                    "test": "system_health",
                    "status": "passed",
                    "details": data
                })
                
        except Exception as e:
            print(f"‚ùå System health check failed: {e}")
            self.test_results.append({
                "test": "system_health", 
                "status": "failed",
                "error": str(e)
            })
    
    async def test_ai_provider_integration(self):
        """Test multi-provider AI integration with different task types"""
        print("\nüß† Test 2: Multi-Provider AI Integration")
        print("-" * 40)
        
        test_scenarios = [
            {
                "task_type": "emotional_support",
                "message": "I'm feeling overwhelmed with my studies and need encouragement",
                "expected_provider": "groq",
                "priority": "speed"
            },
            {
                "task_type": "complex_explanation", 
                "message": "Can you explain quantum computing in detail?",
                "expected_provider": "emergent",
                "priority": "quality"
            },
            {
                "task_type": "quick_response",
                "message": "What is 2+2?",
                "expected_provider": "any",
                "priority": "speed"
            }
        ]
        
        for scenario in test_scenarios:
            await self.test_single_ai_interaction(scenario)
    
    async def test_single_ai_interaction(self, scenario: Dict[str, Any]):
        """Test a single AI interaction scenario"""
        try:
            request_data = {
                "user_id": f"test_user_{scenario['task_type']}",
                "message": scenario["message"],
                "task_type": scenario["task_type"],
                "priority": scenario["priority"],
                "enable_caching": True,
                "max_response_time_ms": 3000
            }
            
            start_time = time.time()
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    response_time = (time.time() - start_time) * 1000
                    
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Extract key metrics
                        ai_response = data.get("response", {})
                        performance = data.get("performance", {})
                        quantum_metrics = data.get("quantum_metrics", {})
                        analytics = data.get("analytics", {})
                        
                        print(f"‚úÖ {scenario['task_type'].upper()} - Response received")
                        print(f"   Provider: {ai_response.get('provider', 'unknown')}")
                        print(f"   Response Time: {response_time:.2f}ms")
                        print(f"   Empathy Score: {ai_response.get('empathy_score', 0):.2f}")
                        print(f"   Confidence: {ai_response.get('confidence', 0):.2f}")
                        print(f"   Quantum Coherence: {quantum_metrics.get('quantum_coherence', 0):.2f}")
                        
                        # Validate empathy for emotional support
                        if scenario['task_type'] == 'emotional_support':
                            empathy_score = ai_response.get('empathy_score', 0)
                            if empathy_score >= 0.7:
                                print(f"   ‚úÖ High empathy detected for emotional support: {empathy_score}")
                            else:
                                print(f"   ‚ö†Ô∏è Low empathy for emotional support: {empathy_score}")
                        
                        # Record performance metrics
                        self.performance_metrics.append({
                            "scenario": scenario['task_type'],
                            "response_time_ms": response_time,
                            "empathy_score": ai_response.get('empathy_score', 0),
                            "confidence": ai_response.get('confidence', 0),
                            "quantum_coherence": quantum_metrics.get('quantum_coherence', 0),
                            "provider": ai_response.get('provider', 'unknown')
                        })
                        
                        self.test_results.append({
                            "test": f"ai_integration_{scenario['task_type']}",
                            "status": "passed", 
                            "response_time_ms": response_time,
                            "metrics": {
                                "empathy_score": ai_response.get('empathy_score', 0),
                                "confidence": ai_response.get('confidence', 0),
                                "quantum_coherence": quantum_metrics.get('quantum_coherence', 0)
                            }
                        })
                        
                    else:
                        error_data = await resp.text()
                        print(f"‚ùå {scenario['task_type'].upper()} - Failed: {resp.status}")
                        print(f"   Error: {error_data}")
                        
                        self.test_results.append({
                            "test": f"ai_integration_{scenario['task_type']}",
                            "status": "failed",
                            "error": f"HTTP {resp.status}: {error_data}"
                        })
                        
        except Exception as e:
            print(f"‚ùå {scenario['task_type'].upper()} - Exception: {e}")
            self.test_results.append({
                "test": f"ai_integration_{scenario['task_type']}",
                "status": "failed",
                "error": str(e)
            })
    
    async def test_real_learning_scenarios(self):
        """Test real-world learning scenarios with different user profiles"""
        print("\nüìö Test 3: Real Learning Scenarios")
        print("-" * 40)
        
        learning_scenarios = [
            {
                "user_id": "struggling_student",
                "message": "I don't understand calculus derivatives at all. I'm getting frustrated.",
                "task_type": "beginner_concepts",
                "expected_difficulty": "easy",
                "expected_empathy": 0.8
            },
            {
                "user_id": "advanced_learner", 
                "message": "Can you explain the mathematical proof behind Fermat's Last Theorem?",
                "task_type": "advanced_concepts",
                "expected_difficulty": "very_challenging",
                "expected_empathy": 0.5
            },
            {
                "user_id": "anxious_student",
                "message": "I have an exam tomorrow and I'm panicking. Nothing makes sense anymore.",
                "task_type": "emotional_support",
                "expected_difficulty": "moderate", 
                "expected_empathy": 0.9
            }
        ]
        
        for scenario in learning_scenarios:
            await self.test_learning_scenario(scenario)
    
    async def test_learning_scenario(self, scenario: Dict[str, Any]):
        """Test individual learning scenario"""
        try:
            request_data = {
                "user_id": scenario["user_id"],
                "message": scenario["message"],
                "task_type": scenario["task_type"],
                "priority": "balanced",
                "enable_caching": True
            }
            
            start_time = time.time()
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    response_time = (time.time() - start_time) * 1000
                    
                    if resp.status == 200:
                        data = await resp.json()
                        
                        # Analyze learning-specific metrics
                        ai_response = data.get("response", {})
                        analytics = data.get("analytics", {})
                        recommendations = data.get("recommendations", {})
                        
                        empathy_score = ai_response.get('empathy_score', 0)
                        adaptation_analysis = analytics.get('adaptation_analysis', {})
                        
                        print(f"‚úÖ {scenario['user_id']} - Learning response generated")
                        print(f"   Empathy Score: {empathy_score:.2f} (expected: {scenario['expected_empathy']})")
                        print(f"   Response Time: {response_time:.2f}ms")
                        print(f"   Personalization: {analytics.get('personalization_score', 0):.2f}")
                        
                        # Validate empathy expectations
                        empathy_diff = abs(empathy_score - scenario['expected_empathy'])
                        if empathy_diff <= 0.3:
                            print(f"   ‚úÖ Empathy level appropriate for scenario")
                        else:
                            print(f"   ‚ö†Ô∏è Empathy mismatch - got {empathy_score}, expected {scenario['expected_empathy']}")
                        
                        # Check for learning recommendations
                        if recommendations.get('learning_suggestions'):
                            print(f"   ‚úÖ Learning suggestions provided")
                        
                        self.test_results.append({
                            "test": f"learning_scenario_{scenario['user_id']}",
                            "status": "passed",
                            "empathy_validation": empathy_diff <= 0.3,
                            "metrics": {
                                "empathy_score": empathy_score,
                                "personalization_score": analytics.get('personalization_score', 0),
                                "response_time_ms": response_time
                            }
                        })
                        
                    else:
                        error_data = await resp.text()
                        print(f"‚ùå {scenario['user_id']} - Failed: {resp.status}")
                        self.test_results.append({
                            "test": f"learning_scenario_{scenario['user_id']}",
                            "status": "failed",
                            "error": f"HTTP {resp.status}: {error_data}"
                        })
                        
        except Exception as e:
            print(f"‚ùå {scenario['user_id']} - Exception: {e}")
            self.test_results.append({
                "test": f"learning_scenario_{scenario['user_id']}",
                "status": "failed",
                "error": str(e)
            })
    
    async def test_performance_metrics(self):
        """Test real-time performance metrics calculation"""
        print("\n‚ö° Test 4: Performance Metrics Validation")
        print("-" * 40)
        
        if not self.performance_metrics:
            print("‚ùå No performance metrics collected from previous tests")
            return
        
        # Analyze collected performance metrics
        response_times = [m['response_time_ms'] for m in self.performance_metrics]
        empathy_scores = [m['empathy_score'] for m in self.performance_metrics]
        quantum_coherence_scores = [m['quantum_coherence'] for m in self.performance_metrics]
        
        avg_response_time = sum(response_times) / len(response_times)
        avg_empathy = sum(empathy_scores) / len(empathy_scores)
        avg_quantum_coherence = sum(quantum_coherence_scores) / len(quantum_coherence_scores)
        
        print(f"üìä Performance Analysis:")
        print(f"   Average Response Time: {avg_response_time:.2f}ms")
        print(f"   Target Response Time: <2000ms")
        print(f"   Average Empathy Score: {avg_empathy:.2f}")
        print(f"   Average Quantum Coherence: {avg_quantum_coherence:.2f}")
        
        # Validate performance targets
        performance_passed = True
        
        if avg_response_time <= 2000:
            print(f"   ‚úÖ Response time meets target")
        else:
            print(f"   ‚ö†Ô∏è Response time exceeds target: {avg_response_time:.2f}ms")
            performance_passed = False
        
        if avg_empathy >= 0.5:
            print(f"   ‚úÖ Empathy scores are healthy")
        else:
            print(f"   ‚ö†Ô∏è Low average empathy: {avg_empathy:.2f}")
            performance_passed = False
        
        if avg_quantum_coherence >= 0.7:
            print(f"   ‚úÖ Quantum coherence is strong")
        else:
            print(f"   ‚ö†Ô∏è Low quantum coherence: {avg_quantum_coherence:.2f}")
            performance_passed = False
        
        self.test_results.append({
            "test": "performance_metrics",
            "status": "passed" if performance_passed else "warning",
            "metrics": {
                "avg_response_time_ms": avg_response_time,
                "avg_empathy_score": avg_empathy,
                "avg_quantum_coherence": avg_quantum_coherence,
                "meets_targets": performance_passed
            }
        })
    
    async def test_empathy_detection(self):
        """Test empathy detection and emotional intelligence"""
        print("\nüí≠ Test 5: Empathy & Emotion Detection")
        print("-" * 40)
        
        empathy_test_cases = [
            {
                "message": "I failed my exam again. I feel like giving up on everything.",
                "expected_empathy_range": (0.8, 1.0),
                "emotion": "sadness/despair"
            },
            {
                "message": "I finally understood that difficult concept! Thank you so much!",
                "expected_empathy_range": (0.6, 0.8),
                "emotion": "joy/excitement"
            },
            {
                "message": "This homework is due tomorrow and I have no idea how to solve it.",
                "expected_empathy_range": (0.7, 0.9),
                "emotion": "stress/anxiety"
            },
            {
                "message": "What is the capital of France?",
                "expected_empathy_range": (0.3, 0.6),
                "emotion": "neutral"
            }
        ]
        
        for i, test_case in enumerate(empathy_test_cases):
            await self.test_empathy_case(i, test_case)
    
    async def test_empathy_case(self, case_num: int, test_case: Dict[str, Any]):
        """Test individual empathy detection case"""
        try:
            request_data = {
                "user_id": f"empathy_test_user_{case_num}",
                "message": test_case["message"],
                "task_type": "emotional_support",
                "priority": "quality"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        empathy_score = data.get("response", {}).get('empathy_score', 0)
                        min_expected, max_expected = test_case["expected_empathy_range"]
                        
                        print(f"‚úÖ Empathy Test {case_num + 1} - {test_case['emotion']}")
                        print(f"   Detected Empathy: {empathy_score:.2f}")
                        print(f"   Expected Range: {min_expected}-{max_expected}")
                        
                        if min_expected <= empathy_score <= max_expected:
                            print(f"   ‚úÖ Empathy level appropriate")
                            empathy_valid = True
                        else:
                            print(f"   ‚ö†Ô∏è Empathy level outside expected range")
                            empathy_valid = False
                        
                        self.test_results.append({
                            "test": f"empathy_detection_{case_num}",
                            "status": "passed" if empathy_valid else "warning",
                            "empathy_score": empathy_score,
                            "expected_range": test_case["expected_empathy_range"],
                            "emotion": test_case["emotion"],
                            "empathy_valid": empathy_valid
                        })
                        
                    else:
                        print(f"‚ùå Empathy Test {case_num + 1} - Failed: {resp.status}")
                        self.test_results.append({
                            "test": f"empathy_detection_{case_num}",
                            "status": "failed",
                            "error": f"HTTP {resp.status}"
                        })
                        
        except Exception as e:
            print(f"‚ùå Empathy Test {case_num + 1} - Exception: {e}")
            self.test_results.append({
                "test": f"empathy_detection_{case_num}",
                "status": "failed",
                "error": str(e)
            })
    
    async def test_difficulty_adaptation(self):
        """Test difficulty adjustment based on user understanding"""
        print("\nüéØ Test 6: Difficulty Adaptation")
        print("-" * 40)
        
        # Test same user with different difficulty signals
        user_id = "difficulty_test_user"
        
        difficulty_scenarios = [
            {
                "message": "I don't understand basic addition at all",
                "expected_difficulty": "very_easy",
                "context": "struggling beginner"
            },
            {
                "message": "That was too easy. Can you give me something more challenging?",
                "expected_difficulty": "challenging",
                "context": "requesting harder content"
            },
            {
                "message": "This is way too hard. I need something simpler to build up.",
                "expected_difficulty": "easy",
                "context": "overwhelmed, needs easier content"
            }
        ]
        
        for i, scenario in enumerate(difficulty_scenarios):
            await self.test_difficulty_scenario(user_id, i, scenario)
    
    async def test_difficulty_scenario(self, user_id: str, scenario_num: int, scenario: Dict[str, Any]):
        """Test individual difficulty adaptation scenario"""
        try:
            request_data = {
                "user_id": user_id,
                "message": scenario["message"],
                "task_type": "adaptive_learning",
                "priority": "quality"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        recommendations = data.get("recommendations", {})
                        difficulty_adjustments = recommendations.get("difficulty_adjustments", {})
                        analytics = data.get("analytics", {})
                        
                        print(f"‚úÖ Difficulty Test {scenario_num + 1} - {scenario['context']}")
                        
                        # Look for difficulty-related signals in the response
                        if difficulty_adjustments:
                            print(f"   Difficulty Adjustments: {difficulty_adjustments}")
                        
                        if analytics.get('adaptation_analysis'):
                            print(f"   Adaptation Analysis Available: ‚úÖ")
                        
                        # Check if learning suggestions are appropriate for difficulty
                        learning_suggestions = recommendations.get("learning_suggestions", [])
                        if learning_suggestions:
                            print(f"   Learning Suggestions: {len(learning_suggestions)} provided")
                        
                        self.test_results.append({
                            "test": f"difficulty_adaptation_{scenario_num}",
                            "status": "passed",
                            "difficulty_adjustments": difficulty_adjustments,
                            "learning_suggestions_count": len(learning_suggestions),
                            "context": scenario["context"]
                        })
                        
                    else:
                        print(f"‚ùå Difficulty Test {scenario_num + 1} - Failed: {resp.status}")
                        self.test_results.append({
                            "test": f"difficulty_adaptation_{scenario_num}",
                            "status": "failed",
                            "error": f"HTTP {resp.status}"
                        })
                        
        except Exception as e:
            print(f"‚ùå Difficulty Test {scenario_num + 1} - Exception: {e}")
            self.test_results.append({
                "test": f"difficulty_adaptation_{scenario_num}",
                "status": "failed",
                "error": str(e)
            })
    
    async def test_context_management(self):
        """Test context management and conversation memory"""
        print("\nüß† Test 7: Context Management")
        print("-" * 40)
        
        # Test conversation context across multiple messages
        user_id = "context_test_user"
        session_id = f"context_session_{int(time.time())}"
        
        conversation_flow = [
            {
                "message": "I'm learning about physics. Can you help me with Newton's laws?",
                "expected_context": "physics_introduction"
            },
            {
                "message": "Can you explain the first law in more detail?",
                "expected_context": "continuing_physics_discussion"
            },
            {
                "message": "How does this relate to everyday examples?",
                "expected_context": "building_on_previous_concepts"
            }
        ]
        
        for i, turn in enumerate(conversation_flow):
            await self.test_context_turn(user_id, session_id, i, turn)
    
    async def test_context_turn(self, user_id: str, session_id: str, turn_num: int, turn: Dict[str, Any]):
        """Test individual conversation turn for context management"""
        try:
            request_data = {
                "user_id": user_id,
                "message": turn["message"],
                "session_id": session_id,
                "task_type": "general",
                "priority": "balanced"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        
                        conversation = data.get("conversation", {})
                        analytics = data.get("analytics", {})
                        
                        print(f"‚úÖ Context Turn {turn_num + 1}")
                        print(f"   Session ID: {conversation.get('session_id', 'N/A')}")
                        print(f"   Message Count: {conversation.get('message_count', 0)}")
                        
                        # Check for context effectiveness
                        context_effectiveness = analytics.get('context_effectiveness', 0)
                        print(f"   Context Effectiveness: {context_effectiveness:.2f}")
                        
                        if context_effectiveness >= 0.7:
                            print(f"   ‚úÖ Strong context management")
                        else:
                            print(f"   ‚ö†Ô∏è Weak context management")
                        
                        self.test_results.append({
                            "test": f"context_management_turn_{turn_num}",
                            "status": "passed",
                            "context_effectiveness": context_effectiveness,
                            "message_count": conversation.get('message_count', 0),
                            "session_id": conversation.get('session_id')
                        })
                        
                    else:
                        print(f"‚ùå Context Turn {turn_num + 1} - Failed: {resp.status}")
                        self.test_results.append({
                            "test": f"context_management_turn_{turn_num}",
                            "status": "failed",
                            "error": f"HTTP {resp.status}"
                        })
                        
        except Exception as e:
            print(f"‚ùå Context Turn {turn_num + 1} - Exception: {e}")
            self.test_results.append({
                "test": f"context_management_turn_{turn_num}",
                "status": "failed",
                "error": str(e)
            })
    
    async def test_optimization_systems(self):
        """Test real-time optimization systems"""
        print("\n‚ö° Test 8: Real-Time Optimization")
        print("-" * 40)
        
        # Test caching optimization
        repeated_message = "What is machine learning?"
        user_id = "optimization_test_user"
        
        # First request (should be slow)
        start_time = time.time()
        first_response = await self.make_test_request(user_id, repeated_message, "general", "speed")
        first_time = (time.time() - start_time) * 1000
        
        # Second request (should be faster due to caching)
        start_time = time.time()
        second_response = await self.make_test_request(user_id, repeated_message, "general", "speed")
        second_time = (time.time() - start_time) * 1000
        
        print(f"üìä Caching Optimization Test:")
        print(f"   First Request: {first_time:.2f}ms")
        print(f"   Second Request: {second_time:.2f}ms")
        
        if second_time < first_time * 0.8:  # 20% improvement expected
            print(f"   ‚úÖ Caching optimization working (20%+ improvement)")
            optimization_working = True
        else:
            print(f"   ‚ö†Ô∏è Limited caching benefit detected")
            optimization_working = False
        
        # Check if optimization flags are present
        if first_response and second_response:
            first_cache = first_response.get("cache_utilized", False)
            second_cache = second_response.get("cache_utilized", False)
            
            print(f"   First Request Cache: {first_cache}")
            print(f"   Second Request Cache: {second_cache}")
        
        self.test_results.append({
            "test": "optimization_systems",
            "status": "passed" if optimization_working else "warning",
            "first_request_ms": first_time,
            "second_request_ms": second_time,
            "optimization_benefit": (first_time - second_time) / first_time * 100,
            "caching_working": optimization_working
        })
    
    async def make_test_request(self, user_id: str, message: str, task_type: str, priority: str) -> Dict[str, Any]:
        """Make a test request and return response data"""
        try:
            request_data = {
                "user_id": user_id,
                "message": message,
                "task_type": task_type,
                "priority": priority,
                "enable_caching": True
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/quantum/message",
                    json=request_data
                ) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        return {}
        except:
            return {}
    
    async def generate_test_report(self):
        """Generate comprehensive test report"""
        print("\n" + "=" * 70)
        print("üìã COMPREHENSIVE QUANTUM INTELLIGENCE TEST REPORT")
        print("=" * 70)
        
        total_tests = len(self.test_results)
        passed_tests = len([t for t in self.test_results if t["status"] == "passed"])
        failed_tests = len([t for t in self.test_results if t["status"] == "failed"])
        warning_tests = len([t for t in self.test_results if t["status"] == "warning"])
        
        print(f"\nüìä OVERALL RESULTS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Passed: {passed_tests} ‚úÖ")
        print(f"   Failed: {failed_tests} ‚ùå")
        print(f"   Warnings: {warning_tests} ‚ö†Ô∏è")
        print(f"   Success Rate: {(passed_tests / total_tests * 100):.1f}%")
        
        # Performance Summary
        if self.performance_metrics:
            response_times = [m['response_time_ms'] for m in self.performance_metrics]
            empathy_scores = [m['empathy_score'] for m in self.performance_metrics]
            quantum_coherence_scores = [m['quantum_coherence'] for m in self.performance_metrics]
            
            print(f"\n‚ö° PERFORMANCE SUMMARY:")
            print(f"   Average Response Time: {sum(response_times)/len(response_times):.2f}ms")
            print(f"   Best Response Time: {min(response_times):.2f}ms")
            print(f"   Worst Response Time: {max(response_times):.2f}ms")
            print(f"   Average Empathy Score: {sum(empathy_scores)/len(empathy_scores):.2f}")
            print(f"   Average Quantum Coherence: {sum(quantum_coherence_scores)/len(quantum_coherence_scores):.2f}")
        
        # Detailed Results
        print(f"\nüîç DETAILED TEST RESULTS:")
        for test in self.test_results:
            status_emoji = "‚úÖ" if test["status"] == "passed" else "‚ùå" if test["status"] == "failed" else "‚ö†Ô∏è"
            print(f"   {status_emoji} {test['test']}: {test['status'].upper()}")
            
            if "error" in test:
                print(f"      Error: {test['error']}")
            
            if "metrics" in test:
                for key, value in test["metrics"].items():
                    if isinstance(value, float):
                        print(f"      {key}: {value:.2f}")
                    else:
                        print(f"      {key}: {value}")
        
        # Save detailed report
        report_data = {
            "timestamp": time.time(),
            "summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "warnings": warning_tests,
                "success_rate": passed_tests / total_tests * 100
            },
            "performance_metrics": self.performance_metrics,
            "detailed_results": self.test_results
        }
        
        with open("/app/quantum_intelligence_test_report.json", "w") as f:
            json.dump(report_data, f, indent=2)
        
        print(f"\nüíæ Detailed report saved to: /app/quantum_intelligence_test_report.json")
        
        # System Recommendations
        print(f"\nüéØ SYSTEM RECOMMENDATIONS:")
        
        if failed_tests == 0:
            print(f"   ‚úÖ All tests passed! System is functioning optimally.")
        else:
            print(f"   ‚ö†Ô∏è {failed_tests} test(s) failed. Review error details above.")
        
        avg_response_time = sum([m['response_time_ms'] for m in self.performance_metrics]) / len(self.performance_metrics) if self.performance_metrics else 0
        if avg_response_time > 2000:
            print(f"   ‚ö° Consider optimizing response times (current: {avg_response_time:.2f}ms)")
        else:
            print(f"   ‚úÖ Response times are within target (<2000ms)")
        
        avg_empathy = sum([m['empathy_score'] for m in self.performance_metrics]) / len(self.performance_metrics) if self.performance_metrics else 0
        if avg_empathy < 0.6:
            print(f"   üí≠ Consider tuning empathy detection algorithms")
        else:
            print(f"   ‚úÖ Empathy detection is performing well")
        
        print("\n" + "=" * 70)
        print("üéâ QUANTUM INTELLIGENCE TESTING COMPLETE")
        print("=" * 70)

async def main():
    """Run comprehensive quantum intelligence testing"""
    validator = QuantumIntelligenceValidator()
    await validator.run_comprehensive_tests()

if __name__ == "__main__":
    asyncio.run(main())